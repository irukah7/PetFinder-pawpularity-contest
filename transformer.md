# Transformerとは
Transformerについて語られている https://qiita.com/omiita/items/07e69aef6c156d23c538 について。

## 1. 論文解説
1.0. 要約
翻訳などの文章を別の文章で出力するというモデル(Sequence Transduction Model)は**Attension**を用いたエンコーダ-デコーダ形式のRNNやCNNが主流であった。
本論文では、RNNやCNNを用いず、Attensionのみを用いたモデルTransformerを提案している。
Transformerの特徴は以下。
* **再帰も畳み込みも一切使わない**
* 翻訳で今までのアンサンブルモデルも含めたSoTAを超えるBLEUスコア(28.4)を叩き出した
* なのに並列化がかなりしやすくて訓練時間が圧倒的に削減できる

1.1. 導入
RNNとエンコーダ-デコーダモデルがこれまでのNLP界を牽引してきたが、逐次的に単語を処理するが故に**訓練時に並列処理ができない**と言う大きな欠点があった。また、長文に対してはAttensionが使われていたが、そのAttensionはほぼRNNと一緒に使われていた。

この論文では、

RNNを一切使わずにAttensionだけで入力と出力の広範囲な依存関係を捉えられるモデル**Transformer**を提案している。

1.2. 背景



#### ※Attension
* 機械学習、深層学習で頻繁に使われるテクニック
* Attension -> 入力データのどの部分に注意を向けるかと言うこと
* あらゆるタスクで従来の性能を上回ってきた
* 最初は自然言語処理で登場したが現在は画像処理分野や動画処理のタスクでも利用されている
* Attensionについての詳しい内容は別記事で書く
