# Transformerとは
Transformerについて語られている https://qiita.com/omiita/items/07e69aef6c156d23c538 について。

## 1. 論文解説
1.0. 要約

翻訳などの文章を別の文章で出力するというモデル(Sequence Transduction Model)は**Attension**を用いたエンコーダ-デコーダ形式のRNNやCNNが主流であった。

本論文では、RNNやCNNを用いず、Attensionのみを用いたモデルTransformerを提案している。

Transformerの特徴は以下。
* **再帰も畳み込みも一切使わない**
* 翻訳で今までのアンサンブルモデルも含めたSoTAを超えるBLEUスコア(28.4)を叩き出した
* なのに並列化がかなりしやすくて訓練時間が圧倒的に削減できる

1.1. 導入

RNNとエンコーダ-デコーダモデルがこれまでのNLP界を牽引してきたが、逐次的に単語を処理するが故に**訓練時に並列処理ができない**と言う大きな欠点があった。また、長文に対してはAttensionが使われていたが、そのAttensionはほぼRNNと一緒に使われていた。

この論文では、RNNを一切使わずにAttensionだけで入力と出力の広範囲な依存関係を捉えられるモデル**Transformer**を提案している。

1.2. 背景

**文章の依存関係を掴むための逐次的な計算を減らす**という目的のもとRNNの代わりに使われたのがCNN。
それによって並列処理をある程度可能にしたものの、文章が長くなるとそれに従って計算量が増えてしまい、より長文の依存関係を掴めない という問題が残った。
ここで登場するのがTransformerである。計算量を文章の長さに応じず定数時間に抑えた。

。。。以下当記事を参考

#### ※Attension
* 機械学習、深層学習で頻繁に使われるテクニック
* Attension -> 入力データのどの部分に注意を向けるかと言うこと
* あらゆるタスクで従来の性能を上回ってきた
* 最初は自然言語処理で登場したが現在は画像処理分野や動画処理のタスクでも利用されている
* Attensionについての詳しい内容はググる

## まとめ
Transformerというのは
* 自然言語処理の分野で使われている深層学習モデルである
* テキストを翻訳したりコードを自動生成したりするように設計されたモデルである
* Transformerを画像認識に応用するモデル**ViT-Vision Transformers**が出てきた
    * Transformer (より正確には、若干構造を変えた Transformer-Encoder)を利用した画像認識モデル
* 今回のPetFinderではTransformerを使用して結構いい精度が出てるらしい
